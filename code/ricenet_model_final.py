# -*- coding: utf-8 -*-
"""RiceNet_Model_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14Y9Wirmn6SJP3eE440YdFiEImd_8N_vm

# Seed classification

**This section describes the dataset, downloads it, extracts images from it and finally splits it further into training and validation sets**

# Dataset Description

**The dataset contains images of five classes of local rice grains namely :**

1. Jehlum

2. Sr-1

3. mushkibudji

4. Sr-2

5. Sr-4

**The dataset has 4900 images in total and is split into :**

* Train, validation and test splits

* Train set contains 3430 images

* validation set contains 735 images

* Test set contains 735 images

**checking version of tensorflow**

# Importing necessary libraries
"""

import numpy as np
import tensorflow as tf
import os
import random
import matplotlib.pyplot as plt
import PIL
import seaborn as sns
import tensorflow.keras as keras
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.initializers import glorot_uniform
from tensorflow.keras.utils import plot_model
from IPython.display import display
from tensorflow.keras import backend as K
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler
from random import sample

print(tf.__version__)

extract_dir = '/data/Data/Dataset'


"""# Split the Dataset into Training and Validation Sets

**The Dataset is split for actual training and validation.**

- The train-validation ratio is 70:30
- Total Images in Dataset = 4900
- Images used for actual training = 3430
- Images used for training validation = 1470
"""

import random
import shutil
# This function splits the image data into train and validation sets
def train_validation_split(dataset_path, output_path, split_ratio, seed=120):
    """Builds the train and validation image sets for all image categories given a split ratio"""
    img_categories = os.listdir(dataset_path)  # all the image categories
    if os.path.exists(output_path):
        print('Dataset already exists at the given path')
    else:
        os.makedirs(output_path)
        os.mkdir(output_path + '/train')
        os.mkdir(output_path + '/validation')

        # for every image category in the dataset build train and val folders with images in them a/c to split_ratio
        print('Splitting dataset into train and validation sets: ')
        for img_category in img_categories:
            print('.', end='')
            # list all the images for this category
            imgs = os.listdir(dataset_path + '/' + img_category)
            # sort and shuffle images randomly
            imgs.sort()
            random.seed(seed)
            random.shuffle(imgs)
            # split the imgs into two halves train and test
            train_split = imgs[:int(split_ratio * len(imgs))]
            test_split=imgs[int(split_ratio * len(imgs)):]

            # built the train set and copy images
            if not os.path.exists(os.path.join(output_path, 'train', img_category)):
                os.mkdir(os.path.join(output_path, 'train', img_category))
            for img in train_split:
                source = os.path.join(dataset_path, img_category, img)
                dest = os.path.join(output_path, 'train', img_category, img)
                shutil.copy(source, dest)

            # built the test set and copy images
            if not os.path.exists(os.path.join(output_path, 'validation', img_category)):
                os.mkdir(os.path.join(output_path, 'validation', img_category))
            for img in test_split:
                source = os.path.join(dataset_path, img_category, img)
                dest = os.path.join(output_path, 'validation', img_category, img)
                shutil.copy(source, dest)

        print('\nSuccess!!')

# Split the training portion into two splits:
 ## 80% for actual training
 ## 20% for validation
dataset_dir='/data/Data/Dataset/items_train_val_set'
t_dir = os.path.join(dataset_dir)
output_path = os.path.join(extract_dir, 'items_train_val_set') # post split path
train_validation_split(t_dir, output_path, 0.7)
train_dir = os.path.join(output_path, 'train')
validation_dir = os.path.join(output_path, 'validation')
# test_dir = os.path.join(dataset_dir, 'test')

"""# Split validation Data into validation and testing Sets

**The validation set is further split for validation and testing the trained model.**

- The validation-test ratio is 50:50
- Total Images in validation Dataset = 1470
- Images used for validation = 735
- Images used for testing= 735
"""

import random
import shutil
# This function splits the image data into train and validation sets
def validation_test_split(dataset_path, output_path, split_ratio, seed=120):
    """Builds the validation and test image sets for all image categories given a split ratio"""
    img_categories = os.listdir(dataset_path)  # all the image categories
    if os.path.exists(output_path):
        print('Dataset already exists at the given path')
    else:
        os.makedirs(output_path)
        os.mkdir(output_path + '/valid')
        os.mkdir(output_path + '/test')

        # for every image category in the dataset build train and val folders with images in them a/c to split_ratio
        print('Splitting dataset into test and validation sets: ')
        for img_category in img_categories:
            print('.', end='')
            # list all the images for this category
            imgs = os.listdir(dataset_path + '/' + img_category)
            # sort and shuffle images randomly
            imgs.sort()
            random.seed(seed)
            random.shuffle(imgs)
            # split the imgs into two halves train and test
            train_split = imgs[:int(split_ratio * len(imgs))]
            test_split=imgs[int(split_ratio * len(imgs)):]

            # built the train set and copy images
            if not os.path.exists(os.path.join(output_path, 'valid', img_category)):
                os.mkdir(os.path.join(output_path, 'valid', img_category))
            for img in train_split:
                source = os.path.join(dataset_path, img_category, img)
                dest = os.path.join(output_path, 'valid', img_category, img)
                shutil.copy(source, dest)

            # built the test set and copy images
            if not os.path.exists(os.path.join(output_path, 'test', img_category)):
                os.mkdir(os.path.join(output_path, 'test', img_category))
            for img in test_split:
                source = os.path.join(dataset_path, img_category, img)
                dest = os.path.join(output_path, 'test', img_category, img)
                shutil.copy(source, dest)

        print('\nSuccess!!')

dataset_dir=output_path
t_dir = os.path.join(dataset_dir, 'validation')
output_path = os.path.join(extract_dir, 'items_val_test_set') # post split path
validation_test_split(t_dir, output_path, 0.5)
validation_dir = os.path.join(output_path, 'valid')
test_dir = os.path.join(output_path, 'test')

"""# Analyze Data Post Split

"""


train_img_categories = os.listdir(train_dir)
validation_img_categories = os.listdir(validation_dir)
test_img_categories = os.listdir(test_dir)

no_of_classes = len(train_img_categories)
print('No of categories/classes: %d\n' % no_of_classes)

print('Object categories in the training set: %d' % len(train_img_categories), end=' ')
print(train_img_categories)
# No of images for each category in the training set
total_train_imgs = 0
for img_cat in train_img_categories:
    path = os.path.join(train_dir, img_cat)
    total_train_imgs += len(os.listdir(path))
    print('No of images for %s = %d' % (img_cat, len(os.listdir(path))))
print('Total no of images in training set = %d\n' % total_train_imgs )

print('Object categories in the validation set: %d' % len(validation_img_categories), end=' ')
print(validation_img_categories)
# No of images for each category in the validation set
total_val_imgs = 0
for img_cat in validation_img_categories:
    path = os.path.join(validation_dir, img_cat)
    total_val_imgs += len(os.listdir(path))
    print('No of images for %s = %d' % (img_cat, len(os.listdir(path))))
print('Total no of images in validation set = %d\n' % total_val_imgs )

print('Object categories in the test set: %d' % len(test_img_categories), end=' ')
print(test_img_categories)
# No of images for each category in the test set
total_test_imgs = 0
for img_cat in test_img_categories:
    path = os.path.join(test_dir, img_cat)
    total_test_imgs += len(os.listdir(path))
    print('No of images for %s = %d' % (img_cat, len(os.listdir(path))))
print('Total no of images in test set = %d' % total_test_imgs )

"""# Visualize Random Images

Some random images are shown from all three sets.

Actual Training, Training Validation and Test set.
"""

# Commented out IPython magic to ensure Python compatibility.
# Show four images of different categories from each of the three sets
# %matplotlib inline

import matplotlib.pyplot as plt
import matplotlib.image as mpimg

random_train_imgs_path = [os.path.join(train_dir, cat, random.choice(os.listdir(os.path.join(train_dir, cat)))) for cat in os.listdir(train_dir)]
random_val_imgs_path = [os.path.join(validation_dir, cat, random.choice(os.listdir(os.path.join(validation_dir, cat)))) for cat in os.listdir(validation_dir)]
random_test_imgs_path = [os.path.join(test_dir, cat, random.choice(os.listdir(os.path.join(test_dir, cat)))) for cat in os.listdir(test_dir)]

# Parameters for graph; output images
nrows = 2
ncols = 3

# Set up matplotlib fig
fig = plt.gcf()
fig.set_size_inches(ncols * 3, nrows * 3)
fig.suptitle('Random image of each category in training set')
for i, img_path in enumerate(random_train_imgs_path):
  # Set up subplot; subplot indices start at 1
  ax = plt.subplot(nrows, ncols, i + 1)
  ax.axis('off')
  img = mpimg.imread(img_path)
  plt.imshow(img)
plt.show()

# Set up matplotlib fig
fig = plt.gcf()
fig.set_size_inches(ncols * 3, nrows * 3)
fig.suptitle('Random image of each category in validation set')
for i, img_path in enumerate(random_val_imgs_path):
  # Set up subplot; subplot indices start at 1
  ax = plt.subplot(nrows, ncols, i + 1)
  ax.axis('off')
  img = mpimg.imread(img_path)
  plt.imshow(img)
plt.show()

# Set up matplotlib fig
fig = plt.gcf()
fig.set_size_inches(ncols * 3, nrows * 3)
fig.suptitle('Random image of each category in test_set')
for i, img_path in enumerate(random_test_imgs_path):
  # Set up subplot; subplot indices start at 1
  ax = plt.subplot(nrows, ncols, i + 1)
  ax.axis('off')
  img = mpimg.imread(img_path)
  plt.imshow(img)
plt.show()

"""# Data Preprocessing

**This section forms the data input pipeline, first images are augmented first, then the images are prepared into batches and resized.**

1. Images are augmented to reduce overfitting and enlargen the small dataset.

2. The images are read from each categories (classes) directory in batches.

# Data Augmentation

**The images are rescaled and augmented.**

1. Augmentation is applied only to the actual training set.

2. Rescaling is applied to both training and validation sets
"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator
# Add our data-augmentation parameters to ImageDataGenerator
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True)

# Note that the validation data should not be augmented!
val_datagen = ImageDataGenerator(rescale=1./255)

from tensorflow.keras.preprocessing.image import array_to_img, img_to_array, load_img

img_path =  [os.path.join(train_dir, cat, random.choice(os.listdir(os.path.join(train_dir, cat)))) for cat in os.listdir(train_dir)][1]
img = load_img(img_path, target_size=(512, 512))  # this is a PIL image
x = img_to_array(img)  # Numpy array with shape (512, 512, 3)
x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 512, 512, 3)

# The .flow() command below generates batches of randomly transformed images
# It will loop indefinitely, so we need to `break` the loop at some point!
i = 0
for batch in train_datagen.flow(x, batch_size=5):
  plt.figure(i)
  imgplot = plt.imshow(array_to_img(batch[0]))
  i += 1
  if i % 5 == 0:
    break

"""# Use ImageDataGenerator class for Augmentation and Rescaling

**The ImageDataGenerator class allows to instantiate generators of augmented image batches (and their labels) via .flow(data, labels) or .flow_from_directory(directory).**

1. Augmentation can be done by configuring a number of random transformations to be performed on the images read by ImageDataGenerator instance.

2. With data augmentation in place, the training images are randomly transformed each time a new training epoch runs, which means that the model will never see the same image twice during training. This helps in preventing overfitting.

3. Augmentation is only applied to the training set

4. Process images by normalizing the pixel values to be in the [0, 1] range (originally all values are in the [0, 255] range).
"""

# define a few parameters for our image dataset
batch_size = 32
image_size = (512, 512)
image_dims = (512, 512, 3) # 3 dimensions rgb
no_classes = 5 # 5 image categories

train_generator = train_datagen.flow_from_directory(
        train_dir, # This is the source directory for training images
        target_size=image_size,  # All images will be resized
        batch_size=batch_size,
        # Since we use categorical_crossentropy loss, use categorical labels
        class_mode='categorical')

# Flow validation images in batches of 20 using validation_datagen generator
validation_generator = val_datagen.flow_from_directory(
        validation_dir,
        target_size=image_size,
        batch_size=batch_size,
        class_mode='categorical')

"""# Build Image Classifier

The model is built using the following layers:
* Conv2D - 2D Convolutional Network
* Activation - Relu
* Batch Normalization
* Max pooling
* Dense - Fully Connected Neural Network
* Softmax Layer as output

**Key Issue in Building ML Models - Overfitting**

Overfitting happens when a model exposed to too few examples learns patterns that do not generalize to new data, i.e. when the model starts using irrelevant features for making predictions

Overfitting is the central problem in machine learning: given that we are fitting the parameters of our model to a given dataset, how can we make sure that the representations learned by the model will be applicable to data never seen before? How do we avoid learning things that are specific to the training data?
"""

from tensorflow.keras import layers
from tensorflow.keras import Model
from tensorflow.keras import regularizers

# Our input feature map is 150x150x3: 150x150 for the image pixels, and 3 for
# the three color channels: R, G, and B
img_input = layers.Input(shape=(150, 150,3))
# Input feature map is 150x150x3: 150x150 for the image pixels, and 3 for
  # the three color channels: R, G, and B
img_input = layers.Input(shape=image_dims)
# First convolution extracts 16 filters that are 3x3
# Batch normalization is applied before activation
# Convolution is followed by max-pooling layer with a 2x2 window
x = layers.Conv2D(16, 5, padding='same')(img_input)
x = layers.BatchNormalization()(x)
x = layers.Activation('relu')(x)
#x = layers.MaxPooling2D(2)(x)

# Second convolution extracts 32 filters that are 3x3
# Batch normalization is applied before activation
# Convolution is followed by max-pooling layer with a 2x2 window
x = layers.Conv2D(16, 5, padding='same')(x)
x = layers.BatchNormalization()(x)
x = layers.Activation('relu')(x)
x = layers.MaxPooling2D(2)(x)


# 3rdconvolution extracts 32 filters that are 3x3
# Batch normalization is applied before activation
# Convolution is followed by max-pooling layer with a 2x2 window
x = layers.Conv2D(32, 3,padding='same')(x)
x = layers.BatchNormalization()(x)
x = layers.Activation('relu')(x)
x = layers.MaxPooling2D(2)(x)

# 4th convolution extracts 32 filters that are 3x3
# Batch normalization is applied before activation
# Convolution is followed by max-pooling layer with a 2x2 window
x = layers.Conv2D(32, 3,padding='same')(x)
x = layers.BatchNormalization()(x)
x = layers.Activation('relu')(x)
x = layers.MaxPooling2D()(x)

# 5th convolution extracts 32 filters that are 3x3
# Batch normalization is applied before activation
# Convolution is followed by max-pooling layer with a 2x2 window
x = layers.Conv2D(64, 3,padding='same')(x)
x = layers.BatchNormalization()(x)
x = layers.Activation('relu')(x)
x = layers.MaxPooling2D(2)(x)

# 6th convolution extracts 32 filters that are 3x3
# Batch normalization is applied before activation
# Convolution is followed by max-pooling layer with a 2x2 window
x = layers.Conv2D(64, 3,padding='same')(x)
x = layers.BatchNormalization()(x)
x = layers.Activation('relu')(x)
x = layers.MaxPooling2D(2)(x)


# 5th convolution extracts 32 filters that are 3x3
# Batch normalization is applied before activation
# Convolution is followed by max-pooling layer with a 2x2 window
x = layers.Conv2D(128, 3,padding='same')(x)
x = layers.BatchNormalization()(x)
x = layers.Activation('relu')(x)
x = layers.MaxPooling2D(2)(x)

# Convolution is followed by max-pooling layer with a 2x2 window
x = layers.Conv2D(128, 3,padding='same')(x)
x = layers.BatchNormalization()(x)
x = layers.Activation('relu')(x)
x = layers.MaxPooling2D(2)(x)


# Convolution is followed by max-pooling layer with a 2x2 window
x = layers.Conv2D(128, 3,padding='same')(x)
x = layers.BatchNormalization()(x)
x = layers.Activation('relu')(x)
x = layers.MaxPooling2D(2)(x)

# Convolution is followed by max-pooling layer with a 2x2 window
x = layers.Conv2D(192, 3,padding='same')(x)
x = layers.BatchNormalization()(x)
x = layers.Activation('relu')(x)
x = layers.MaxPooling2D(2)(x)


# Flatten feature map to a 1-dim tensor to add fully connected layers
x = layers.Flatten()(x)

# Create a fully connected layer with ReLU activation and 512 hidden units
# Batch normalization is applied before activation
x = layers.Dense(192)(x)
x = layers.BatchNormalization()(x)
x = layers.Activation('relu')(x)

# Add a dropout rate of 0.5 to prevent overfitting
x = layers.Dropout(0.55)(x)

# Create output layer using softmax activation
output = layers.Dense(no_classes, activation='softmax')(x)

# Create model:
# input = input feature map
# output = input feature map + stacked convolution/maxpooling layers + fully
# connected layer + softmax output layer
simple_model = Model(img_input, output)

# Summarize the model
simple_model.summary()

import numpy as np
import random
from tensorflow.keras.preprocessing.image import img_to_array, load_img

# Define a new Model that will take an image as input, and will output
# intermediate representations for all layers in the previous model after
# the first.
successive_outputs = [layer.output for layer in simple_model.layers[1:]]
visualization_model = Model(img_input, successive_outputs)

# Prepare a random input image of each category/class from the training set.
random_images1 = [os.path.join(train_dir, cat, random.choice(os.listdir(os.path.join(train_dir, cat)))) for cat in os.listdir(train_dir)]
img_path = random.choice(np.array(random_images1).flatten())

img = load_img(img_path, target_size=(512, 512))  # this is a PIL image
x = img_to_array(img)  # Numpy array with shape (150, 150, 3)
x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)
# Rescale by 1/255
x /= 255

# Let's run our image through our network, thus obtaining all
# intermediate representations for this image.
successive_feature_maps = visualization_model.predict(x)

# These are the names of the layers, so can have them as part of our plot
layer_names = [layer.name for layer in simple_model.layers]

# Now let's display our representations
for layer_name, feature_map in zip(layer_names, successive_feature_maps):
  if len(feature_map.shape) == 4:
    # Just do this for the conv / maxpool layers, not the fully-connected layers
    n_features = feature_map.shape[-1]  # number of features in feature map
    # The feature map has shape (1, size, size, n_features)
    size = feature_map.shape[1]
    # We will tile our images in this matrix
    display_grid = np.zeros((size, size * n_features))
    for i in range(n_features):
      # Postprocess the feature to make it visually palatable
      x = feature_map[0, :, :, i]
      x -= x.mean()
      x /= x.std()
      x *= 64
      x += 128
      x = np.clip(x, 0, 255).astype('uint8')
      # Tile each filter into this big horizontal grid
      display_grid[:, i * size : (i + 1) * size] = x
    # Display the grid
    scale = 20. / n_features
    plt.figure(figsize=(scale * n_features, scale))
    plt.title(layer_name)
    plt.grid(False)
    plt.imshow(display_grid, aspect='auto')

"""# Compile Model

**The model is compiled with training parameters set**

Next, we'll configure the specifications for model training. We will train our model with the binary_crossentropy loss, because it's a binary classification problem and our final activation is a sigmoid. (For a refresher on loss metrics, see the Machine Learning Crash Course.) We will use the rmsprop optimizer with a learning rate of 0.00003. During training, we will want to monitor classification accuracy.
"""

from tensorflow.keras.optimizers import Adam
simple_model.compile(loss='categorical_crossentropy',
              optimizer=Adam(
                  lr=0.00003),
              metrics=['acc'])

import tensorflow as tf
# print(tf.__version__)
config = tf.compat.v1.ConfigProto(device_count = {'GPU': 0})
sess = tf.compat.v1.Session(config=config)

"""# Train Model

The model is trained by feeding the data from the image generators
"""

from keras.callbacks import ModelCheckpoint
from PIL import Image, ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True
checkpoint_path = "training_8/cp.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,monitor='val_acc', save_best_only=True, mode='auto',
                                                 verbose=1)
simple_model_history = simple_model.fit_generator(
        train_generator,
        steps_per_epoch=total_train_imgs // batch_size,  # total_train_images = batch_size * steps
        epochs=1,
        callbacks= [cp_callback],
        validation_data=validation_generator,
        validation_steps=total_val_imgs // batch_size,  # total_val_images = batch_size * steps
        verbose=2)

"""# Loading the saved weights of the our trained model"""

# Loads the weights
checkpoint_path="training_7/training_7/cp.ckpt"
simple_model.load_weights(checkpoint_path)

"""# Confusion Matrix"""

import itertools
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        title='Normalized confusion matrix'
    else:
        title='Confusion matrix'

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()

"""# Roc Curve"""

from scipy import interp
from sklearn.metrics import roc_curve, auc
## ROC (Receiver Operating Characteristic)
## ROC Curve for Multiclass Classification:
  ## ROC curves are typically used in binary classification to study the output of a classifier.
  ## In order to extend ROC curve and ROC area to multi-class or multi-label classification, it is necessary to binarize the output.
  ## One ROC curve can be drawn per label, or draw a ROC curve by considering each element of the label indicator matrix as a binary prediction (micro-averaging).
  ## Another evaluation measure for multi-class classification is macro-averaging, which gives equal weight to the classification of each label.
def plot_roc_curve(n_classes, y_true, y_pred):
  lw = 2
  # Compute ROC curve and ROC area for each class
  fpr = dict()
  tpr = dict()
  roc_auc = dict()
  for i in range(n_classes):
      fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_pred[:, i])
      roc_auc[i] = auc(fpr[i], tpr[i])

  # Compute micro-average ROC curve and ROC area
  fpr["micro"], tpr["micro"], _ = roc_curve(y_true.ravel(), y_pred.ravel())
  roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

  # Compute macro-average ROC curve and ROC area

  # First aggregate all false positive rates
  all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))

  # Then interpolate all ROC curves at this points
  mean_tpr = np.zeros_like(all_fpr)
  for i in range(n_classes):
      mean_tpr += interp(all_fpr, fpr[i], tpr[i])

  # Finally average it and compute AUC
  mean_tpr /= n_classes

  fpr["macro"] = all_fpr
  tpr["macro"] = mean_tpr
  roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

  # Plot all ROC curves
  plt.figure()
  plt.plot(fpr["micro"], tpr["micro"],
           label='micro-average ROC curve (area = {0:0.2f})'
                 ''.format(roc_auc["micro"]),
           color='red', linestyle=':', linewidth=4)

  plt.plot(fpr["macro"], tpr["macro"],
           label='macro-average ROC curve (area = {0:0.2f})'
                 ''.format(roc_auc["macro"]),
           color='green', linestyle=':', linewidth=4)

  colors = itertools.cycle(['black', 'blue', 'yellow', 'brown'])
  for i, color in zip(range(n_classes), colors):
      plt.plot(fpr[i], tpr[i], color=color, lw=lw,
               label='ROC curve of class {0} (area = {1:0.2f})'
               ''.format(i, roc_auc[i]))
  plt.plot([0, 1], [0, 1], 'k--', lw=lw)
  plt.xlim([0.0, 1.0])
  plt.ylim([0.0, 1.05])
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive Rate')
  plt.title('Some extension of Receiver operating characteristic to multi-class')
  plt.legend(loc="lower right")
  plt.show()

"""# Classification Report"""

from sklearn.metrics import classification_report

from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
## multiclass or binary report
## If binary (sigmoid output), set binary parameter to True
def full_multiclass_report(model,
                           y_pred,
                           y_true,
                           classes,
                           batch_size=32,
                           binary=False):

    # 1. Transform one-hot encoded y_true & y_pred into their class numbers
    if not binary:
        y_true_classes = np.argmax(y_true,axis=1)
        y_pred_classes = np.argmax(y_pred,axis=1)

    # 2. Print accuracy score
    print("Accuracy : "+ str(accuracy_score(y_true_classes,y_pred_classes)))

    print("")

    # 3. Print classification report
    print("Classification Report")
    print(classification_report(y_true_classes,y_pred_classes,digits=5))

    # 4. Plot confusion matrix
    cnf_matrix = confusion_matrix(y_true_classes,y_pred_classes)
    plot_confusion_matrix(cnf_matrix,classes=classes)

    # 5. Plot ROC Curve
    plot_roc_curve(len(classes), y_true, y_pred)

# !pip install --upgrade tensorflow

# function used to convert test images
from tensorflow.keras.preprocessing import image
def convert_image(img_path):
    # dimensions of our images
    img = image.load_img(img_path, target_size=image_size)
    img_tensor = image.img_to_array(img) # (height, width, channels)
    # (1, height, width, channels), add a dimension because the model expects this shape: (batch_size, height, width, channels)
    img_tensor = np.expand_dims(img_tensor, axis=0)
    img_tensor /= 255 # imshow expects values in the range [0, 1]
    return img_tensor

"""# Run Test set on Model

**Show accuracy, confusion matrix and roc curve for each model**
"""

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
def test_model(model):
  # predict classes for each image of every category in the test set
  classes = train_generator.class_indices
  actual_results={}
  test_results={}

  for img_cat in os.listdir(test_dir):
      test_images_path = os.path.join(test_dir, img_cat)
      actual_results[img_cat] = []
      test_results[img_cat] = []
      for img in os.listdir(test_images_path):
          actual_results[img_cat].append(img_cat)
          img_path = os.path.join(test_images_path, img)
          new_image = convert_image(img_path)
          # check predictions
          class_predictions = model.predict(new_image)
          most_likely_class_index = np.argmax(class_predictions,axis=1)
          predicted_class = list(classes.keys())[list(classes.values()).index(most_likely_class_index)]
          test_results[img_cat].append(predicted_class)

  for cat in test_results:
      n_images = len(test_results[cat])
      true = 0
      for predicted_cat in test_results[cat]:
          if predicted_cat == cat:
              true +=1
      acc = round(true / n_images, 2)
      print('The %% accuracy for %s is %0.2f' % (cat, 100*acc))


  label_encoder = LabelEncoder()
  onehot_encoder = OneHotEncoder(sparse=False)

  y_pred = np.array(list(test_results.values()))

  # integer encode
  integer_encoded = label_encoder.fit_transform(y_pred.flatten())

  # binary encode
  integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)
  onehot_encoded = onehot_encoder.fit_transform(integer_encoded)

  y_pred = onehot_encoded

  y_true = np.array(list(actual_results.values()))

  #integer encode
  integer_encoded = label_encoder.fit_transform(y_true.flatten())

  # binary encode
  integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)
  onehot_encoded = onehot_encoder.fit_transform(integer_encoded)

  y_true = onehot_encoded

  full_multiclass_report(model, y_pred, y_true, list(classes.keys()))

print('Testing Simple Model')
test_model(simple_model)
print('\n-----------------------\n')

"""# Saving our results in results folder"""

import sys
original_stdout = sys.stdout
with open('/results/output', 'w') as f:
     sys.stdout = f
     print('Testing Simple Model')
     test_model(simple_model)
     print('\\n-----------------------\\n')
sys.stdout = original_stdout

"""# END"""